{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SashkaHacker/BigData/blob/main/labs/%D0%9B%D0%A06_%D0%9E%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B0_%D0%B8_%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D1%8F_%D1%82%D0%B5%D0%BA%D1%81%D1%82%D0%BE%D0%B2%D1%8B%D1%85_%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Выполнил:** студент 4 курса, группы ПИЖ-б-о-22-1, Матвеев Александр Иванович"
      ],
      "metadata": {
        "id": "iLY9RDMtHPQy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Постановка задачи\n"
      ],
      "metadata": {
        "id": "Mx1V6rkZLGNJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Цель работы:** применение наивного байесовского классификатора для определения тональности текстов на русском языке.\n",
        "\n",
        "**Краткое описание:** в ходе выполнения лабораторной работы необходимо провести предобработку сырых текстовых данных и настроить гиперпараметры нескольких векторайзеров так, чтобы точность классификации (`accuracy`) наивным байесовским классификатором из `sklearn` составила не менее 0.74."
      ],
      "metadata": {
        "id": "BPzgy4tKLP-x"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjvL0IIG8Jd0"
      },
      "source": [
        "# Данные\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaSxXHvxO0qo"
      },
      "source": [
        "В папке [Data/lab6](https://drive.google.com/drive/folders/1jo_V1RYyFKNa2TR_VNZYMGKUi3T1Jpze?usp=sharing) расположена таблица с русскоязычными текстовыми отзывами на предметы одежды. Также эти же данные можно найти [по сслыке](https://github.com/sismetanin/rureviews). Все задания лабораторной работы необходимо выполнять по этим данным."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Формат данных**\n",
        "\n",
        "```\n",
        "review\tsentiment\n",
        "Очень плохо упакован, весь  мятый и смотрится не так как на фото\tnegative\n",
        "заказ не пришёл но деньги вернули\tneautral\n",
        "Мягкая удобная тёплая. Все замечательно. Ничего не торчит запахов нет. Продавцу + в карму\tpositive\n",
        "...\n",
        "```\n",
        "##### **Описание полей**\n",
        "\n",
        " - ```review``` — сырой текст отзыва в большинстве случаев на русском языке, встречаются отдельные записи на английском;\n",
        " - ```sentiment``` — тональность отзыва (позитивный, нейтральный, негативный), классы сбалансированы."
      ],
      "metadata": {
        "id": "XSwSCVWsNr-W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Практические задания"
      ],
      "metadata": {
        "id": "B6OlR-CoqO3E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Предварительная обработка данных"
      ],
      "metadata": {
        "id": "Dd85JlatxPaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Токенизация"
      ],
      "metadata": {
        "id": "RFngH4gi5oCv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D39SSh0zWg-r"
      },
      "source": [
        "Токенизировать – значит разделить текст на части: слова, ключевые слова, фразы, символы и т.д., иными словами – **токены**.\n",
        "\n",
        "Самый простой способ токенизировать текст – разделить с помощью метода `split()`. Но `split` упускает очень много всего, например, не отделяет пунктуацию от слов. Кроме этого, есть ещё много менее тривиальных проблем, поэтому лучше использовать готовые токенизаторы, к примеру:\n",
        "\n",
        "- `Yargy` парсер. Документация: https://github.com/natasha/yargy\n",
        "- Парсер из модуля `nltk`. Учебник по nltk: https://www.nltk.org/book\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1w3Ic3iWEWVl",
        "outputId": "9bc04a34-6d9e-439b-fbc0-06f58c602eac"
      },
      "source": [
        "# токенизация через split()\n",
        "\n",
        "text = '''\n",
        "Продаётся LADA 4x4. ПТС 01.12.2018, куплена 20 января 19 года, 10 000 км пробега.\n",
        "Комплектация полная. Новая в салоне 750 000, отдам за 650 000.\n",
        "Возможен обмен на ВАЗ-2110 или ВАЗ 2109 с вашей доплатой.\n",
        "Краснодар, ул. Миклухо-Маклая, д. 4/5, подьезд 1\n",
        "Тел. 8(999)1234567, 8 903 987-65-43, +7 (351) 111 22 33\n",
        "И.И. Иванов (Иван Иванович)\n",
        "'''\n",
        "\n",
        "tokens = text.split()\n",
        "print(f\"split() определил {len(tokens)} токенов\")\n",
        "print(tokens)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "split() определил 56 токенов\n",
            "['Продаётся', 'LADA', '4x4.', 'ПТС', '01.12.2018,', 'куплена', '20', 'января', '19', 'года,', '10', '000', 'км', 'пробега.', 'Комплектация', 'полная.', 'Новая', 'в', 'салоне', '750', '000,', 'отдам', 'за', '650', '000.', 'Возможен', 'обмен', 'на', 'ВАЗ-2110', 'или', 'ВАЗ', '2109', 'с', 'вашей', 'доплатой.', 'Краснодар,', 'ул.', 'Миклухо-Маклая,', 'д.', '4/5,', 'подьезд', '1', 'Тел.', '8(999)1234567,', '8', '903', '987-65-43,', '+7', '(351)', '111', '22', '33', 'И.И.', 'Иванов', '(Иван', 'Иванович)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLORuVHIEowR"
      },
      "source": [
        "%%capture\n",
        "# токенизация через yargy\n",
        "!pip install yargy"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2FbXdmZEWQf",
        "outputId": "7d45d332-97a1-473a-d2c2-431a6183dcc1"
      },
      "source": [
        "from yargy.tokenizer import MorphTokenizer\n",
        "\n",
        "tknzr = MorphTokenizer()\n",
        "tokens = [_.value for _ in tknzr(text)]\n",
        "print(f\"yargy определил {len(tokens)} токенов\")\n",
        "print(tokens)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yargy определил 107 токенов\n",
            "['\\n', 'Продаётся', 'LADA', '4', 'x', '4', '.', 'ПТС', '01', '.', '12', '.', '2018', ',', 'куплена', '20', 'января', '19', 'года', ',', '10', '000', 'км', 'пробега', '.', '\\n', 'Комплектация', 'полная', '.', 'Новая', 'в', 'салоне', '750', '000', ',', 'отдам', 'за', '650', '000', '.', '\\n', 'Возможен', 'обмен', 'на', 'ВАЗ', '-', '2110', 'или', 'ВАЗ', '2109', 'с', 'вашей', 'доплатой', '.', '\\n', 'Краснодар', ',', 'ул', '.', 'Миклухо', '-', 'Маклая', ',', 'д', '.', '4', '/', '5', ',', 'подьезд', '1', '\\n', 'Тел', '.', '8', '(', '999', ')', '1234567', ',', '8', '903', '987', '-', '65', '-', '43', ',', '+', '7', '(', '351', ')', '111', '22', '33', '\\n', 'И', '.', 'И', '.', 'Иванов', '(', 'Иван', 'Иванович', ')', '\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoSe08N2Wg-r"
      },
      "source": [
        "%%capture\n",
        "# токенизация через nltk\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrJDGpgYWg-4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "523e4a66-d9b5-4cbc-fab6-85ca56053462"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "print(f\"nltk определил {len(tokens)} токенов\")\n",
        "print(tokens)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nltk определил 81 токенов\n",
            "['Продаётся', 'LADA', '4x4', '.', 'ПТС', '01.12.2018', ',', 'куплена', '20', 'января', '19', 'года', ',', '10', '000', 'км', 'пробега', '.', 'Комплектация', 'полная', '.', 'Новая', 'в', 'салоне', '750', '000', ',', 'отдам', 'за', '650', '000', '.', 'Возможен', 'обмен', 'на', 'ВАЗ-2110', 'или', 'ВАЗ', '2109', 'с', 'вашей', 'доплатой', '.', 'Краснодар', ',', 'ул', '.', 'Миклухо-Маклая', ',', 'д', '.', '4/5', ',', 'подьезд', '1', 'Тел', '.', '8', '(', '999', ')', '1234567', ',', '8', '903', '987-65-43', ',', '+7', '(', '351', ')', '111', '22', '33', 'И.И', '.', 'Иванов', '(', 'Иван', 'Иванович', ')']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_702Dg5OWg-5"
      },
      "source": [
        "В `nltk` есть довольно много токенизаторов. Токенизатор подбирается, исходя из языка и  требований задачи."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ps8oPYoTWg-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf72a894-dcae-460e-d712-7cb00d3b8f1f"
      },
      "source": [
        "from nltk import tokenize\n",
        "\n",
        "print(f\"Количество токенизаторов в nltk: {len(dir(tokenize))}\")\n",
        "dir(tokenize)[:5]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Количество токенизаторов в nltk: 61\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['BlanklineTokenizer',\n",
              " 'LegalitySyllableTokenizer',\n",
              " 'LineTokenizer',\n",
              " 'MWETokenizer',\n",
              " 'NLTKWordTokenizer']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Очистка текста"
      ],
      "metadata": {
        "id": "9HnGmSSg5rtG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0Nco1mJnaB_"
      },
      "source": [
        "Очистка текста включает в себя несколько этапов:\n",
        "- удаление пунктуации;\n",
        "- преобразование регистра;\n",
        "- удаление стоп-слов.\n",
        "\n",
        "Сюда же включается исправление опечаток и этапы очистки от лишней информации (тегов html-разметки, url, непонятных символов и др.) и приведения отдельных потенциально важных элементов текстов к единому виду (времени или дат, например)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KT-maq-T0ZVM",
        "outputId": "5c804b73-f8c4-4552-f72b-1a4530110089"
      },
      "source": [
        "# набор пунктуационных символов зависит от задачи и текста\n",
        "import string\n",
        "string.punctuation"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWgbvwqZ1vUv",
        "outputId": "c8a0a1c1-b71d-4c5b-c811-65f60ec73f6c"
      },
      "source": [
        "# удаление стандартной пунктуации\n",
        "punct = string.punctuation\n",
        "clean_words = [w.strip(punct) for w in word_tokenize(text)]\n",
        "print(clean_words)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Продаётся', 'LADA', '4x4', '', 'ПТС', '01.12.2018', '', 'куплена', '20', 'января', '19', 'года', '', '10', '000', 'км', 'пробега', '', 'Комплектация', 'полная', '', 'Новая', 'в', 'салоне', '750', '000', '', 'отдам', 'за', '650', '000', '', 'Возможен', 'обмен', 'на', 'ВАЗ-2110', 'или', 'ВАЗ', '2109', 'с', 'вашей', 'доплатой', '', 'Краснодар', '', 'ул', '', 'Миклухо-Маклая', '', 'д', '', '4/5', '', 'подьезд', '1', 'Тел', '', '8', '', '999', '', '1234567', '', '8', '903', '987-65-43', '', '7', '', '351', '', '111', '22', '33', 'И.И', '', 'Иванов', '', 'Иван', 'Иванович', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rf9mZiDw5N_R",
        "outputId": "ac6d03d2-3682-4ed5-9aa1-474c3877ac33"
      },
      "source": [
        "# преобразование регистра\n",
        "clean_words = [w.lower() for w in clean_words if w != '']\n",
        "print(clean_words)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['продаётся', 'lada', '4x4', 'птс', '01.12.2018', 'куплена', '20', 'января', '19', 'года', '10', '000', 'км', 'пробега', 'комплектация', 'полная', 'новая', 'в', 'салоне', '750', '000', 'отдам', 'за', '650', '000', 'возможен', 'обмен', 'на', 'ваз-2110', 'или', 'ваз', '2109', 'с', 'вашей', 'доплатой', 'краснодар', 'ул', 'миклухо-маклая', 'д', '4/5', 'подьезд', '1', 'тел', '8', '999', '1234567', '8', '903', '987-65-43', '7', '351', '111', '22', '33', 'и.и', 'иванов', 'иван', 'иванович']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Z36NVzC6G7q"
      },
      "source": [
        "**Стоп-слова** — высокочастотные слова, которые не дают никакой информации о конкретном тексте. Они составляют верхушку частотного списка в любом языке. Набор стоп-слов не универсален, он будет зависеть от задачи. Обычно начинают с готовых списков стоп-слов, а потом их дополняют."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CX2r-VcI6SjW",
        "outputId": "c2b05756-1bd8-432c-89dc-aa9fc6e25af7"
      },
      "source": [
        "# список стоп-слов для русского языка в nltk\n",
        "from nltk.corpus import stopwords\n",
        "sw = stopwords.words('russian')\n",
        "print(f\"Количество стоп-слов: {len(sw)}\")\n",
        "print(sw)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Количество стоп-слов: 151\n",
            "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZJU5_9X6ZJz",
        "outputId": "975c558d-4bcf-4077-a3b7-1af090fcb7b8"
      },
      "source": [
        "# удаление стоп-слов\n",
        "print([w if w not in sw else print(w) for w in clean_words])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "в\n",
            "за\n",
            "на\n",
            "или\n",
            "с\n",
            "['продаётся', 'lada', '4x4', 'птс', '01.12.2018', 'куплена', '20', 'января', '19', 'года', '10', '000', 'км', 'пробега', 'комплектация', 'полная', 'новая', None, 'салоне', '750', '000', 'отдам', None, '650', '000', 'возможен', 'обмен', None, 'ваз-2110', None, 'ваз', '2109', None, 'вашей', 'доплатой', 'краснодар', 'ул', 'миклухо-маклая', 'д', '4/5', 'подьезд', '1', 'тел', '8', '999', '1234567', '8', '903', '987-65-43', '7', '351', '111', '22', '33', 'и.и', 'иванов', 'иван', 'иванович']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Нормализация текста"
      ],
      "metadata": {
        "id": "0rk9xWtH52VX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-2xAVYN5Og8"
      },
      "source": [
        "**Нормализация токенов** (token normalization) – это процесс приведения токенов к канонической форме, чтобы устранить несущественные различия между последовательностями символов.\n",
        "\n",
        "Для многих задач естественно рассматривать как отдельный признак каждое *слово*, а не каждую его отдельную форму.\n",
        "\n",
        "**Стемминг** (англ. stemming — находить происхождение) — это процесс нахождения основы слова для заданного исходного слова. Основа слова не обязательно совпадает с морфологическим корнем слова.\n",
        "\n",
        "**Лемматизация** – это сведение разных форм одного слова к начальной форме – **лемме**.\n",
        "\n",
        "**Пример:**\n",
        "\n",
        "Лексема `saw` после стемминга может стать буквой `s`, а в ходе лемматизации либо словом `see`, либо `saw`, в зависимости от того, глагол это или существительное."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Стемминг"
      ],
      "metadata": {
        "id": "SQerbFPTCtLG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpjcfN3W5YIA"
      },
      "source": [
        "Стемминг — отсечение от слова окончаний и суффиксов, чтобы оставшаяся часть, называемая stem, была одинаковой для всех грамматических форм слова. Стем необязательно совпадает с морфлогической основой слова. Одинаковый стем может получиться и не у однокоренных слов и наоборот — в этом проблема стемминга.\n",
        "\n",
        "1-ый вид ошибки: белый, белка, белье $\\implies$ бел\n",
        "\n",
        "2-ой вид ошибки: трудность, трудный $\\implies$ трудност, труд\n",
        "\n",
        "3-ий вид ошибки: быстрый, быстрее $\\implies$ быст, побыстрее $\\implies$ побыст\n",
        "\n",
        "Самый простой алгоритм, [алгоритм Портера](https://ru.wikipedia.org/wiki/%D0%A1%D1%82%D0%B5%D0%BC%D0%BC%D0%B5%D1%80_%D0%9F%D0%BE%D1%80%D1%82%D0%B5%D1%80%D0%B0), состоит из 5 циклов команд, на каждом цикле – операция удаления / замены суффикса. Возможны вероятностные расширения алгоритма."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wifIWi9s50Bx"
      },
      "source": [
        "**Snowball stemmer** - улучшенный вариант стеммера Портера; в отличие от него умеет работать не только с английским текстом."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mugfnmz553dy",
        "outputId": "b1b384a1-8ec5-4d80-85bc-f9c3f295267a"
      },
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "SnowballStemmer.languages"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('arabic',\n",
              " 'danish',\n",
              " 'dutch',\n",
              " 'english',\n",
              " 'finnish',\n",
              " 'french',\n",
              " 'german',\n",
              " 'hungarian',\n",
              " 'italian',\n",
              " 'norwegian',\n",
              " 'porter',\n",
              " 'portuguese',\n",
              " 'romanian',\n",
              " 'russian',\n",
              " 'spanish',\n",
              " 'swedish')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSIcGFCT6D00"
      },
      "source": [
        "poem = '''\n",
        "По морям, играя, носится\n",
        "с миноносцем миноносица.\n",
        "Льнет, как будто к меду осочка,\n",
        "к миноносцу миноносочка.\n",
        "И конца б не довелось ему,\n",
        "благодушью миноносьему.\n",
        "Вдруг прожектор, вздев на нос очки,\n",
        "впился в спину миноносочки.\n",
        "Как взревет медноголосина:\n",
        "Р-р-р-астакая миноносина!\n",
        "'''\n",
        "\n",
        "words = [w.strip(punct).lower() for w in word_tokenize(poem)]\n",
        "words = [w for w in words if w not in sw and w != '']"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8sLaPQU6pC8",
        "outputId": "5cd2ee2b-6e3f-475c-9f7a-4eb65001dd54"
      },
      "source": [
        "snowball = SnowballStemmer(\"russian\")\n",
        "\n",
        "for w in words:\n",
        "    print(\"%s: %s\" % (w, snowball.stem(w)))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "морям: мор\n",
            "играя: игр\n",
            "носится: нос\n",
            "миноносцем: миноносц\n",
            "миноносица: миноносиц\n",
            "льнет: льнет\n",
            "меду: мед\n",
            "осочка: осочк\n",
            "миноносцу: миноносц\n",
            "миноносочка: миноносочк\n",
            "конца: конц\n",
            "б: б\n",
            "довелось: довел\n",
            "благодушью: благодуш\n",
            "миноносьему: минонос\n",
            "прожектор: прожектор\n",
            "вздев: вздев\n",
            "нос: нос\n",
            "очки: очк\n",
            "впился: впил\n",
            "спину: спин\n",
            "миноносочки: миноносочк\n",
            "взревет: взревет\n",
            "медноголосина: медноголосин\n",
            "р-р-р-астакая: р-р-р-астак\n",
            "миноносина: миноносин\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Лемматизация"
      ],
      "metadata": {
        "id": "myFvu0STDM2I"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdCH2oNJ6ufM"
      },
      "source": [
        "Лемматизация — процесс приведения словоформы к лемме, т.е. нормальной (словарной) форме. Это более сложная задача, чем стемминг, но и результаты дает гораздо более осмысленные, особенно для языков с богатой морфологией.\n",
        "\n",
        "* кошке, кошку, кошкам, кошкой $\\implies$ кошка\n",
        "* бежал, бежит, бегу $\\implies$ бежать\n",
        "* белому, белым, белыми $\\implies$ белый\n",
        "\n",
        "[Pymorphy 2](http://pymorphy2.readthedocs.io/en/latest/) — полноценный морфологический анализатор, целиком написанный на Python. Для русского языка используются словари OpenCorpora и [граммемы](https://pymorphy2.readthedocs.io/en/latest/user/grammemes.html), принятые в OpenCorpora (с небольшими изменениями)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXSZQBqF-2ED"
      },
      "source": [
        "%%capture\n",
        "!pip install pymorphy2"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lr3QSuKI_NLh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ea900ce-53e0-49ae-c2ba-34768145637a"
      },
      "source": [
        "from pymorphy2 import MorphAnalyzer\n",
        "pymorphy2_analyzer = MorphAnalyzer()\n",
        "\n",
        "for w in words:\n",
        "    print(\"%s: %s :%s\" % (w, snowball.stem(w), pymorphy2_analyzer.parse(w)[0].normal_form))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "морям: мор :море\n",
            "играя: игр :играть\n",
            "носится: нос :носиться\n",
            "миноносцем: миноносц :миноносец\n",
            "миноносица: миноносиц :миноносица\n",
            "льнет: льнет :льнуть\n",
            "меду: мед :мёд\n",
            "осочка: осочк :осочок\n",
            "миноносцу: миноносц :миноносец\n",
            "миноносочка: миноносочк :миноносочек\n",
            "конца: конц :конец\n",
            "б: б :б\n",
            "довелось: довел :довестись\n",
            "благодушью: благодуш :благодушие\n",
            "миноносьему: минонос :миноносий\n",
            "прожектор: прожектор :прожектор\n",
            "вздев: вздев :вздеть\n",
            "нос: нос :нос\n",
            "очки: очк :очки\n",
            "впился: впил :впиться\n",
            "спину: спин :спина\n",
            "миноносочки: миноносочк :миноносочек\n",
            "взревет: взревет :взреветь\n",
            "медноголосина: медноголосин :медноголосина\n",
            "р-р-р-астакая: р-р-р-астак :р-р-р-астакать\n",
            "миноносина: миноносин :миноносина\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Векторизация текста"
      ],
      "metadata": {
        "id": "tAmFMupi55nO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPEecD4MZb3-"
      },
      "source": [
        "Допустим, мы хотим применить вот такую модель для классификации текстов. Например, хотим определять спам и не спам.\n",
        "$$ \\hat{y} = \\psi (w_1 x_1 + w_2 x_2 + ... + w_k x_k + b)$$\n",
        "\n",
        "$\\hat{y} = \\lbrace spam, ham \\rbrace$\n",
        "\n",
        "$w_i$ и $b$ - это параметры, которые нужно найти в процессе обучения (настройки, fitting) модели.\n",
        "\n",
        "А что такое для текстов $x_i$? Нам нужно получить преобразование текстов в численный вектор, с которым может работать стандартный алгоритм машинного обучения. Как это сделать?\n",
        "\n",
        "Тут может быть много разных ответов. Самая простая идея - это 0 или 1, где 0 - если соответствующего токена нет в тексте, а 1 - если есть."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rAdzyI8c_ve"
      },
      "source": [
        "#### Мешок n-грамм"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqg4u1RyduqG"
      },
      "source": [
        "Самые мелкие структуры языка, с которыми можно работать, называются **n-граммами**.\n",
        "У n-граммы есть параметр n - количество слов, которые попадают в такое представление текста.\n",
        "* Если n = 1 - то смотрим на то, сколько раз каждое слово встретилось в тексте. Получаются униграммы.\n",
        "* Если n = 2 - то смотрим на то, сколько раз каждая пара подряд идущих слов, встретилась в тексте. Получаются биграммы.\n",
        "\n",
        "Численное представление текста получается путем подсчета количества n-грамм в текстах. Так как результат не зависит от порядка слов в текстах, то говорят, что такая модель представления текстов в виде векторов получается из *гипотезы представления текста как мешка слов*.\n",
        "\n",
        "Прежде чем получать n-граммы, нужно разделить текст на токены и провести все необходимые для задачи этапы предобработки текста. Векторизация – последний этап перед построением модели."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pue_fX5d8u6W"
      },
      "source": [
        "Самый простой способ извлечь признаки из текстовых данных – использовать класс `CountVectorizer` из `sklearn`.\n",
        "\n",
        "Объект `CountVectorizer` делает следующее:\n",
        "* строит для каждого документа (каждой пришедшей ему строки) вектор размерности `n`, где `n` – количество слов или n-грам во всём корпусе;\n",
        "* заполняет каждый i-тый элемент количеством вхождений слова в данный документ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ar-a8zLZ8rFL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "6ff91e26-c5e1-4eba-b197-d0efa8dc67a6"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# инициализация векторайзера с униграммами, биграммами и триграммами\n",
        "vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
        "\n",
        "# fit_transform сначала обучает векторайзер, а потом сразу применает его к набору данных\n",
        "vectorized_text = vectorizer.fit_transform([\" \".join(tokens)])\n",
        "\n",
        "# первые 5 объектов вектора\n",
        "pd.DataFrame(vectorized_text.T.todense(), index=vectorizer.get_feature_names_out(), columns=[\"n-gram score\"]).head()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                    n-gram score\n",
              "000                            3\n",
              "000 возможен                   1\n",
              "000 возможен обмен             1\n",
              "000 км                         1\n",
              "000 км пробега                 1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-35987823-cde0-4cce-b925-ab7c0ea9e711\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>n-gram score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>000</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>000 возможен</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>000 возможен обмен</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>000 км</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>000 км пробега</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-35987823-cde0-4cce-b925-ab7c0ea9e711')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-35987823-cde0-4cce-b925-ab7c0ea9e711 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-35987823-cde0-4cce-b925-ab7c0ea9e711');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-5fc76689-39ad-4b0c-ab14-68f666b6d73d\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5fc76689-39ad-4b0c-ab14-68f666b6d73d')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-5fc76689-39ad-4b0c-ab14-68f666b6d73d button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"n-gram score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoGFq3_r7FQJ"
      },
      "source": [
        "#### Мешок символьных n-gram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2mnhKPO7hU3"
      },
      "source": [
        "В некоторых задачах в качестве признаков могут быть использованы n-граммы символов. Для этого необходимо установить в ```CountVectorizer()``` параметр ```analyzer = 'char'```, то есть анализировать символы."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "O1bwUYLi7lZS",
        "outputId": "e7fd0162-f771-4bca-d69d-9ef27d350346"
      },
      "source": [
        "# инициализация векторайзера для символов\n",
        "char_vectorizer = CountVectorizer(analyzer='char', ngram_range=(2, 8))\n",
        "\n",
        "# обучаем его и сразу применяем тексту\n",
        "char_vectorized_text = char_vectorizer.fit_transform([\" \".join(tokens)])\n",
        "\n",
        "# первые 5 объектов вектора\n",
        "pd.DataFrame(char_vectorized_text.T.todense(), index=char_vectorizer.get_feature_names_out(), columns=[\"Char n-gram score\"]).head()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Char n-gram score\n",
              "(                      3\n",
              "(                      3\n",
              "( 3                    1\n",
              "( 35                   1\n",
              "( 351                  1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4c70fa25-8eeb-438d-8ae3-cb83771f5682\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Char n-gram score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>(</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>( 3</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>( 35</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>( 351</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4c70fa25-8eeb-438d-8ae3-cb83771f5682')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4c70fa25-8eeb-438d-8ae3-cb83771f5682 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4c70fa25-8eeb-438d-8ae3-cb83771f5682');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-e069f535-a863-4501-84ec-b700c72dd91c\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e069f535-a863-4501-84ec-b700c72dd91c')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-e069f535-a863-4501-84ec-b700c72dd91c button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Char n-gram score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMkbMZh59qEz"
      },
      "source": [
        "Cимвольные n-граммы используются, например, для задачи определения языка. Ещё одна замечательная особенность признаков-символов – для них не нужна токенизация и лемматизация, можно использовать такой подход для языков, у которых нет готовых анализаторов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlWxW3e9Wg-m"
      },
      "source": [
        "#### TF-IDF векторизация"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7hCxZRtWg-m"
      },
      "source": [
        "`TfidfVectorizer` делает то же, что и `CountVectorizer`, но в качестве значений выдает **TF-IDF** каждого слова.\n",
        "\n",
        "**TF (term frequency)** – относительная частотность слова в документе:\n",
        "$$ TF(t,d) = \\frac{n_{t}}{\\sum_k n_{k}} $$\n",
        "\n",
        "**IDF (inverse document frequency)** – обратная частота документов, в которых есть это слово:\n",
        "$$ IDF(t, D) = \\mbox{log} \\frac{|D|}{|{d : t \\in d}|} $$\n",
        "\n",
        "Перемножаем их:\n",
        "$$TFIDF(t, d, D) = TF(t,d) \\times IDF(i, D)$$\n",
        "\n",
        "Идея в том, что если слово часто встречается в одном документе, но в целом по корпусу встречается в небольшом\n",
        "количестве документов, у него высокий TF-IDF."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02f_zZm14PHM"
      },
      "source": [
        "Для получения векторного представления текста с помощью TF-IDF удобно использовать `TfidfVectorizer()`, действующий, как ```CountVectorizer()```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fv7DfTkJWg-n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "0b748477-e34d-4cf5-f4b4-38b5ebe19a31"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# инициализация векторайзера для символов\n",
        "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 4))\n",
        "\n",
        "# обучаем его и сразу применяем тексту\n",
        "tfidf_vectorized_text = tfidf_vectorizer.fit_transform([\" \".join(tokens)])\n",
        "\n",
        "# первые 5 объектов вектора\n",
        "pd.DataFrame(tfidf_vectorized_text.T.todense(), index=tfidf_vectorizer.get_feature_names_out(), columns=[\"TF-IDF\"]).head()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                         TF-IDF\n",
              "000                    0.201347\n",
              "000 возможен           0.067116\n",
              "000 возможен обмен     0.067116\n",
              "000 возможен обмен на  0.067116\n",
              "000 км                 0.067116"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ee383abd-b32b-4ae5-b002-afa57099aea7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TF-IDF</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>000</th>\n",
              "      <td>0.201347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>000 возможен</th>\n",
              "      <td>0.067116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>000 возможен обмен</th>\n",
              "      <td>0.067116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>000 возможен обмен на</th>\n",
              "      <td>0.067116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>000 км</th>\n",
              "      <td>0.067116</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ee383abd-b32b-4ae5-b002-afa57099aea7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ee383abd-b32b-4ae5-b002-afa57099aea7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ee383abd-b32b-4ae5-b002-afa57099aea7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-e7150d0d-d5d5-4c13-a0a6-a776a08b5afb\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e7150d0d-d5d5-4c13-a0a6-a776a08b5afb')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-e7150d0d-d5d5-4c13-a0a6-a776a08b5afb button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"TF-IDF\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06003002251876642,\n        \"min\": 0.06711560552140243,\n        \"max\": 0.2013468165642073,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.06711560552140243,\n          0.2013468165642073\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Помимо диапазона n-грамм, в каждом из трех векторайзеров можно в качестве гиперпараметра передать максимальное и минимальное число вхождения n-граммы в документ (`max_df` и `min_df`), а также максимальное количество используемых из словаря n-грамм (`max_features`). При настройке гиперпараметров векторайзеров их тоже следует учитывать."
      ],
      "metadata": {
        "id": "JTqrvJGSOdRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 1\n",
        "\n",
        "Для обработки текста выполните следующие шаги:\n",
        "- токенизация;\n",
        "- приведение к нижнему регистру;\n",
        "- удаление стоп-слов (с расширением стандартного списка);\n",
        "- очистка текста с помощью регулярных выражений;\n",
        "- нормализация (лемматизация или стемминг);\n",
        "- векторизация (с настройкой гиперпараметров).\n",
        "\n",
        "Обязательными этапами предобработки являются **токенизация** и **векторизация**. Необходимо ли включать остальные шаги, нужно решить в процессе настройки модели. Порядок выполнения шагов предобработки тоже может отличаться от приведенного в списке.\n",
        "\n",
        "Обязательно использование векторайзеров:\n",
        "1. Мешок n-грамм.\n",
        "2. TF-IDF.\n",
        "3. Символьные n-граммы.\n",
        "\n",
        "Для каждого из векторайзеров нужно настроить значения следующих гиперпараметров:\n",
        "- `ngram_range`;\n",
        "- `max_df`;\n",
        "- `min_df`;\n",
        "- `max_features`."
      ],
      "metadata": {
        "id": "G0COoQQ99Nlk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Импорт необходимых библиотек\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "\n",
        "# Для обработки текста\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "# Для лемматизации\n",
        "import pymorphy2\n",
        "\n",
        "# Для векторизации\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# Для классификации\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Для визуализации\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Загрузка данных NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "1Qt4x_h-9cOx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1cf64c7-3ef9-467f-9496-1b4617cbebd8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1VT7Kt3XMVH_f_-Qib19gMmg_u6N1LlXQ"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaPuasAWJdMs",
        "outputId": "61a98660-73f4-4235-ee90-7f295ae2cf61"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1VT7Kt3XMVH_f_-Qib19gMmg_u6N1LlXQ\n",
            "To: /content/women-clothing-accessories.csv\n",
            "100% 21.8M/21.8M [00:00<00:00, 123MB/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка данных\n",
        "df = pd.read_csv('women-clothing-accessories.csv', sep='\\t')\n",
        "\n",
        "# Просмотр первых строк\n",
        "print(f\"Размер данных: {df.shape}\")\n",
        "print(\"\\nПервые 5 строк:\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\nРаспределение классов:\")\n",
        "print(df['sentiment'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvNAwcLtI1qR",
        "outputId": "426ebd21-0b57-4d2a-e6e8-2a9c51ad3a42"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Размер данных: (90000, 2)\n",
            "\n",
            "Первые 5 строк:\n",
            "                                              review sentiment\n",
            "0  качество плохое пошив ужасный (горловина напер...  negative\n",
            "1  Товар отдали другому человеку, я не получила п...  negative\n",
            "2  Ужасная синтетика! Тонкая, ничего общего с пре...  negative\n",
            "3  товар не пришел, продавец продлил защиту без м...  negative\n",
            "4      Кофточка голая синтетика, носить не возможно.  negative\n",
            "\n",
            "Распределение классов:\n",
            "sentiment\n",
            "negative    30000\n",
            "neautral    30000\n",
            "positive    30000\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Инициализация инструментов\n",
        "stop_words = set(stopwords.words('russian'))\n",
        "# Добавление дополнительных стоп-слов\n",
        "additional_stopwords = {'это', 'также', 'еще', 'уже', 'все', 'всё', 'свой', 'своя', 'свои', 'наш', 'ваш', 'который', 'которые'}\n",
        "stop_words.update(additional_stopwords)\n",
        "\n",
        "stemmer = SnowballStemmer(\"russian\")\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "def preprocess_text(text, method='lemmatization'):\n",
        "    \"\"\"\n",
        "    Функция предобработки текста\n",
        "    method: 'lemmatization' или 'stemming'\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # 1. Удаление лишних символов с помощью регулярных выражений\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # удаление URL\n",
        "    text = re.sub(r'[@#]\\w+', '', text)  # удаление упоминаний и хештегов\n",
        "    text = re.sub(r'\\d+', '', text)  # удаление цифр\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)  # удаление пунктуации, кроме пробелов\n",
        "\n",
        "    # 2. Токенизация\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    # 3. Удаление стоп-слов и коротких слов\n",
        "    tokens = [word for word in tokens if word not in stop_words and len(word) > 2]\n",
        "\n",
        "    # 4. Нормализация\n",
        "    if method == 'lemmatization':\n",
        "        tokens = [morph.parse(word)[0].normal_form for word in tokens]\n",
        "    elif method == 'stemming':\n",
        "        tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Применение предобработки\n",
        "df['processed_review'] = df['review'].apply(lambda x: preprocess_text(x, method='lemmatization'))\n",
        "\n",
        "print(\"Пример обработки текста:\")\n",
        "print(\"Оригинал:\", df['review'].iloc[0])\n",
        "print(\"Обработанный:\", df['processed_review'].iloc[0])\n",
        "print(\"\\nКоличество обработанных текстов:\", len(df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcfrdLOAJ3mx",
        "outputId": "a8022618-6350-42b7-8aeb-beb7676eaa02"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Пример обработки текста:\n",
            "Оригинал: качество плохое пошив ужасный (горловина наперекос) Фото не соответствует Ткань ужасная рисунок блеклый маленький рукав не такой УЖАС!!!!! не стоит за такие деньги г.......\n",
            "Обработанный: качество плохой пошив ужасный горловина наперекос фото соответствовать ткань ужасный рисунок блёклый маленький рукав ужас стоить такой деньга\n",
            "\n",
            "Количество обработанных текстов: 90000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Построение модели"
      ],
      "metadata": {
        "id": "OfNni6WcQXXJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Наивный байесовский классификатор"
      ],
      "metadata": {
        "id": "QnVb7XXrQcmT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQKELQ8lrqzB"
      },
      "source": [
        "Рассмотрим задачу классификации.\n",
        "\n",
        "Есть объект $x^{(i)}$, для него нужно оценить вероятность того, что его метка класса - это $k$. Это можно записать с использованием условной вероятности.\n",
        "\n",
        "$$P(y^{(i)}=k|x^{(i)})$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$P(y^{(i)}=k) != P(y^{(i)}=k|x^{(i)})$$"
      ],
      "metadata": {
        "id": "V0otaDxQmqka"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6h6f34Qt_uk"
      },
      "source": [
        "***Определение***\n",
        "Условной вероятностью $A$ при условии $B$, называется $P(A|B)=\\frac{P(AB)}{P(B)}$. Здесь $P(AB)$ — вероятность одновременного наступления событий $A$ и $B$, а $P(B)>0$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATMqpa1ssmOZ"
      },
      "source": [
        "Для оценки этой вероятности воспользуемся теоремой Байеса, которая говорит нам о том, что апостериорная вероятность - прямо пропорциональна произведению априорной вероятности на правдоподобие."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQqIQo6Ku2nq"
      },
      "source": [
        "***Теорема (формула) Байеса***\n",
        "\n",
        "$$P(A|B)=\\frac{P(B|A)\\,P(A)}{P(B)}$$ при $P(B) > 0$\n",
        "\n",
        "Заметим, что если использовать формулу полной вероятности, то можно записать формулу Байеса в виде:\n",
        "\n",
        "$$P(A_i|B)=\\frac{P(B|A_i)\\,P(A_i)}{\\sum_{i=1}^{n}P(B|A_i)P(A_i)}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EizTdmNmwhLI"
      },
      "source": [
        "> Как формула Байеса связана с задачей классификации?\n",
        "\n",
        "Пусть $X$ множество объектов, $Y$ конечное множество имён классов,\n",
        "множество $X \\times Y$ является вероятностным пространством с плотностью распределения $p(x,y)=p(y)p(x|y)$.\n",
        "Вероятности появления объектов каждого из классов $P_y=p(y)$ называются ''априорными вероятностями классов''.\n",
        "Плотности распределения $p_y(x)=p(x|y)$ называются ''функциями правдоподобия классов''.\n",
        "\n",
        "По известным плотностям распределения $p_y(x)$ и априорным вероятностям $P_y$ всех классов $y \\in Y$ строят алгоритм $a(x)$, минимизирующий вероятность ошибочной классификации.\n",
        "\n",
        "По формуле Байеса:\n",
        "\n",
        "$$p(y|x) = \\frac{p(x|y)\\,p(y)}{p(x)}$$\n",
        "\n",
        "По формуле полной вероятности:\n",
        "$$ p(x) = \\sum_{y_i} (p(x|y_i)p(y_i)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AD-o3KZjwtCb"
      },
      "source": [
        "> Откуда взять эти самые \"априорные вероятности классов\" и \"функции правдоподобия классов\"?\n",
        "\n",
        "Априорные вероятности классов $P_y$ можно оценить согласно закону больших чисел, тогда частота появления объектов каждого из классов равна $P'_y=\\frac{l_y}{l}$ где $l_y=|X^l_y|, y \\in Y$ сходится по вероятности к $P_y$ при $l_y \\to \\infty$. Чем больше длина выборки, тем точнее выборочная оценка $P'_y$.\n",
        "\n",
        "Функции правдоподобия нам неизвестны, на самом деле, но по данным мы можем построить их эмпирические оценки."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Для применения наивного байесовского классификатора при работе с текстом в `sklearn` существует `MultinomialNB`."
      ],
      "metadata": {
        "id": "9TeFSPO1U2N-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Оценка качества классификации"
      ],
      "metadata": {
        "id": "aaOX9d_CQf9p"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_qn9NrqeW6w"
      },
      "source": [
        "**Точность** (Accuracy) показывает долю правильных ответов классификатора от общего числа предсказаний.\n",
        "\n",
        "**Точность** (Precision, Positive Predictive Value) отражает какой процент положительных объектов (т.е. тех, что мы считаем положительными) правильно классифицирован.\n",
        "\n",
        "**Полнота** (Sensitivity, True Positive Rate, Recall, Hit Rate) отражает какой процент объектов положительного класса мы правильно классифицировали.\n",
        "\n",
        "Легко построить алгоритм со 100%-й полнотой: он все объекты относит к классу 1, но при этом точность может быть очень низкой. Нетрудно построить алгоритм с близкой к 100% точностью: он относит к классу 1 только те объекты, в которых уверен, при этом полнота может быть низкая.\n",
        "\n",
        "**F1-мера** (F1 score) является средним гармоническим точности и полноты, максимизация этого функционала приводит к одновременной максимизации этих двух «ортогональных критериев»\n",
        "\n",
        "$$F_1 = \\frac{2}{\\mathrm{recall}^{-1} + \\mathrm{precision}^{-1}} = 2 \\cdot \\frac{\\mathrm{precision} \\cdot \\mathrm{recall}}{\\mathrm{precision} + \\mathrm{recall}} = \\frac{\\mathrm{tp}}{\\mathrm{tp} + \\frac12 (\\mathrm{fp} + \\mathrm{fn}) } $$\n",
        "\n",
        "Также рассматривают весовое среднее гармоническое точности и полноты –  $F_\\beta$-меру:\n",
        "\n",
        "$$F_\\beta = (1 + \\beta^2) \\cdot \\frac{\\mathrm{precision} \\cdot \\mathrm{recall}}{(\\beta^2 \\cdot \\mathrm{precision}) + \\mathrm{recall}} = \\frac {(1 + \\beta^2) \\cdot \\mathrm{tp} }{(1 + \\beta^2) \\cdot \\mathrm{tp} + \\beta^2 \\cdot \\mathrm{fn} + \\mathrm{fp}}\\,$$\n",
        "\n",
        "Изменение $\\beta$ позволяет делать один из критериев (точность или полноту) важнее при оптимизации.\n",
        "\n",
        "> Для расчета сразу нескольких метрик качества классификации удобно воспользоваться функцией `classification_report()` из `sklearn.metrics`."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 2"
      ],
      "metadata": {
        "id": "9ujoOf_LQRCs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJox-LoonoPx"
      },
      "source": [
        "По предобработанным данным постройте несколько моделей по разным векторайзерам с разными гиперпараметрами и оцените качество.\n",
        "\n",
        "В качестве классификатора нужно использовать наивный байесовский классификатор.\n",
        "\n",
        "Для сравнения моделей с разными векторайзерами между собой используйте метрики `precision`, `recall`, `f1-score` и `accuracy`. Для этого сформируйте таблицу (датафрейм на `pandas`), в котором в строках будут разные векторайзеры, а в столбцах разные метрики качества, а в  ячейках будут значения этих метрик для соответсвующих векторайзеров. Другими словами, таблица должна иметь следующий вид:\n",
        "\n",
        "Векторайзер и его параметры | recall | precision | f1-score | accuracy\n",
        "--- | --- | --- | --- | ---\n",
        "```Векторайзер 1``` |\n",
        "```Векторайзер 2``` |\n",
        "```Векторайзер 3``` |\n",
        "... |\n",
        "```Векторайзер N``` |\n",
        "\n",
        "\n",
        "Лучшая модель должна работать с точностью (`accuracy`) не менее 0.74."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Разделение данных на обучающую и тестовую выборки\n",
        "X = df['processed_review']\n",
        "y = df['sentiment']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"Размер обучающей выборки: {len(X_train)}\")\n",
        "print(f\"Размер тестовой выборки: {len(X_test)}\")\n",
        "print(f\"\\nКоличество примеров по классам:\")\n",
        "print(y_train.value_counts())\n",
        "print(\"\\nСоотношение классов:\")\n",
        "print(y_train.value_counts(normalize=True))\n",
        "\n",
        "# Создаем словарь для хранения результатов\n",
        "results = []\n",
        "\n",
        "def evaluate_vectorizer(vectorizer, name, X_train, X_test, y_train, y_test):\n",
        "    \"\"\"\n",
        "    Улучшенная функция оценки векторайзера\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Векторизация\n",
        "        X_train_vec = vectorizer.fit_transform(X_train)\n",
        "        X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "        # Проверка, что у нас есть признаки\n",
        "        if X_train_vec.shape[1] == 0:\n",
        "            print(f\"Предупреждение: {name} не создал ни одного признака\")\n",
        "            return None\n",
        "\n",
        "        print(f\"  Создано признаков: {X_train_vec.shape[1]}\")\n",
        "        print(f\"  Размерность данных: {X_train_vec.shape}\")\n",
        "\n",
        "        # Обучение модели с настройкой alpha\n",
        "        model = MultinomialNB(alpha=1.0)  # Начинаем с alpha=1.0\n",
        "        model.fit(X_train_vec, y_train)\n",
        "\n",
        "        # Предсказание\n",
        "        y_pred = model.predict(X_test_vec)\n",
        "\n",
        "        # Оценка качества\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        report = classification_report(y_test, y_pred, output_dict=True)\n",
        "\n",
        "        # Подсчет метрик для каждого класса\n",
        "        result = {\n",
        "            'vectorizer': name,\n",
        "            'accuracy': accuracy,\n",
        "            'num_features': X_train_vec.shape[1]\n",
        "        }\n",
        "\n",
        "        # Добавляем метрики по классам\n",
        "        for class_name in report:\n",
        "            if class_name in ['accuracy', 'macro avg', 'weighted avg']:\n",
        "                continue\n",
        "            result[f'precision_{class_name}'] = report[class_name]['precision']\n",
        "            result[f'recall_{class_name}'] = report[class_name]['recall']\n",
        "            result[f'f1_{class_name}'] = report[class_name]['f1-score']\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при оценке {name}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Расширенные конфигурации векторайзеров\n",
        "configurations = [\n",
        "    # ===== COUNT VECTORIZER =====\n",
        "    # Более агрессивные настройки\n",
        "    (CountVectorizer(ngram_range=(1, 1), max_df=0.95, min_df=1, max_features=10000), \"Count (1-1, max_df=0.95)\"),\n",
        "    (CountVectorizer(ngram_range=(1, 1), max_df=0.8, min_df=5, max_features=8000), \"Count (1-1, min_df=5)\"),\n",
        "\n",
        "    # С биграммами\n",
        "    (CountVectorizer(ngram_range=(1, 2), max_df=0.9, min_df=2, max_features=15000), \"Count (1-2, max_features=15000)\"),\n",
        "    (CountVectorizer(ngram_range=(2, 2), max_df=0.95, min_df=3, max_features=5000), \"Count (2-2, only bigrams)\"),\n",
        "\n",
        "    # С триграммами\n",
        "    (CountVectorizer(ngram_range=(1, 3), max_df=0.85, min_df=2, max_features=20000), \"Count (1-3, max_features=20000)\"),\n",
        "    (CountVectorizer(ngram_range=(2, 3), max_df=0.9, min_df=3, max_features=10000), \"Count (2-3, no unigrams)\"),\n",
        "\n",
        "    # ===== TF-IDF VECTORIZER =====\n",
        "    (TfidfVectorizer(ngram_range=(1, 1), max_df=0.95, min_df=1, max_features=10000), \"TF-IDF (1-1)\"),\n",
        "    (TfidfVectorizer(ngram_range=(1, 1), max_df=0.7, min_df=3, max_features=5000, sublinear_tf=True), \"TF-IDF (1-1, sublinear)\"),\n",
        "\n",
        "    (TfidfVectorizer(ngram_range=(1, 2), max_df=0.9, min_df=2, max_features=15000), \"TF-IDF (1-2)\"),\n",
        "    (TfidfVectorizer(ngram_range=(1, 2), max_df=0.8, min_df=5, max_features=10000, sublinear_tf=True), \"TF-IDF (1-2, sublinear)\"),\n",
        "\n",
        "    (TfidfVectorizer(ngram_range=(1, 3), max_df=0.85, min_df=2, max_features=20000), \"TF-IDF (1-3)\"),\n",
        "\n",
        "    # ===== CHAR N-GRAMS =====\n",
        "    (CountVectorizer(analyzer='char', ngram_range=(2, 4), max_df=0.99, min_df=1, max_features=10000), \"Char (2-4)\"),\n",
        "    (CountVectorizer(analyzer='char', ngram_range=(3, 5), max_df=0.95, min_df=2, max_features=8000), \"Char (3-5)\"),\n",
        "    (CountVectorizer(analyzer='char', ngram_range=(3, 6), max_df=0.9, min_df=3, max_features=15000), \"Char (3-6)\"),\n",
        "    (CountVectorizer(analyzer='char', ngram_range=(4, 7), max_df=0.85, min_df=4, max_features=5000), \"Char (4-7)\"),\n",
        "\n",
        "    # Комбинация символьных и словесных\n",
        "    (CountVectorizer(analyzer='char_wb', ngram_range=(3, 5), max_df=0.95, min_df=1, max_features=12000), \"Char wb (3-5)\"),\n",
        "]\n",
        "\n",
        "# Дополнительно: попробуем разные альфа для Naive Bayes\n",
        "def evaluate_with_different_alphas(vectorizer_configs):\n",
        "    \"\"\"Оценка с разными значениями alpha для MultinomialNB\"\"\"\n",
        "    all_results = []\n",
        "\n",
        "    for vectorizer, name in vectorizer_configs:\n",
        "        print(f\"\\nОценка {name} с разными alpha...\")\n",
        "\n",
        "        # Векторизация\n",
        "        X_train_vec = vectorizer.fit_transform(X_train)\n",
        "        X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "        for alpha in [0.1, 0.5, 1.0, 2.0, 5.0]:\n",
        "            model = MultinomialNB(alpha=alpha)\n",
        "            model.fit(X_train_vec, y_train)\n",
        "            y_pred = model.predict(X_test_vec)\n",
        "            accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "            result = {\n",
        "                'vectorizer': f\"{name} (alpha={alpha})\",\n",
        "                'accuracy': accuracy,\n",
        "                'alpha': alpha,\n",
        "                'num_features': X_train_vec.shape[1]\n",
        "            }\n",
        "            all_results.append(result)\n",
        "\n",
        "            print(f\"  alpha={alpha}: accuracy={accuracy:.4f}\")\n",
        "\n",
        "    return all_results\n",
        "\n",
        "# Оценка всех конфигураций\n",
        "print(\"Начинаем оценку конфигураций...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for vectorizer, name in configurations:\n",
        "    print(f\"\\nОценка {name}...\")\n",
        "    result = evaluate_vectorizer(vectorizer, name, X_train, X_test, y_train, y_test)\n",
        "    if result:\n",
        "        results.append(result)\n",
        "        print(f\"  Accuracy: {result['accuracy']:.4f}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# Дополнительная оценка с разными alpha для лучших конфигураций\n",
        "print(\"\\n\\nДОПОЛНИТЕЛЬНАЯ ОПТИМИЗАЦИЯ: оценка с разными alpha\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Выберем 3 лучшие конфигурации на основе предварительных результатов\n",
        "if results:\n",
        "    # Сортируем по accuracy и берем лучшие\n",
        "    temp_df = pd.DataFrame(results)\n",
        "    best_configs = []\n",
        "\n",
        "    # Берем лучший из каждого типа\n",
        "    for vec_type in ['Count', 'TF-IDF', 'Char']:\n",
        "        type_results = temp_df[temp_df['vectorizer'].str.contains(vec_type)]\n",
        "        if not type_results.empty:\n",
        "            best_idx = type_results['accuracy'].idxmax()\n",
        "            best_name = type_results.loc[best_idx, 'vectorizer']\n",
        "\n",
        "            # Находим соответствующую конфигурацию\n",
        "            for vec, name in configurations:\n",
        "                if name == best_name:\n",
        "                    best_configs.append((vec, f\"{name}_optimized\"))\n",
        "                    print(f\"Выбрана для оптимизации: {name}\")\n",
        "                    break\n",
        "\n",
        "    # Оцениваем с разными alpha\n",
        "    alpha_results = evaluate_with_different_alphas(best_configs)\n",
        "    results.extend(alpha_results)\n",
        "\n",
        "# Создание итоговой таблицы\n",
        "if results:\n",
        "    results_df = pd.DataFrame(results)\n",
        "\n",
        "    # Сортируем по accuracy\n",
        "    results_df = results_df.sort_values('accuracy', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"ИТОГОВЫЕ РЕЗУЛЬТАТЫ (топ-10):\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Показываем топ-10 результатов\n",
        "    top_results = results_df.head(10)[['vectorizer', 'accuracy', 'num_features']]\n",
        "    print(top_results.to_string())\n",
        "\n",
        "    # Лучший результат\n",
        "    best_result = results_df.iloc[0]\n",
        "    print(f\"\\n🎯 ЛУЧШИЙ РЕЗУЛЬТАТ:\")\n",
        "    print(f\"Векторайзер: {best_result['vectorizer']}\")\n",
        "    print(f\"Точность (accuracy): {best_result['accuracy']:.4f}\")\n",
        "    print(f\"Количество признаков: {best_result['num_features']}\")\n",
        "\n",
        "    # Детализация по классам для лучшего результата\n",
        "    print(f\"\\n📊 Детализация по классам для лучшего векторайзера:\")\n",
        "    for col in results_df.columns:\n",
        "        if col.startswith('precision_') or col.startswith('recall_') or col.startswith('f1_'):\n",
        "            if col in best_result:\n",
        "                class_name = col.split('_')[1]\n",
        "                metric = col.split('_')[0]\n",
        "                if metric == 'precision':\n",
        "                    print(f\"  {class_name}: Precision = {best_result[col]:.3f}\")\n",
        "                elif metric == 'recall':\n",
        "                    print(f\"  {class_name}: Recall = {best_result[col]:.3f}\")\n",
        "                elif metric == 'f1':\n",
        "                    print(f\"  {class_name}: F1-score = {best_result[col]:.3f}\")\n",
        "\n",
        "    # Проверка достижения цели\n",
        "    if best_result['accuracy'] >= 0.74:\n",
        "        print(f\"\\n✅ ЦЕЛЬ ДОСТИГНУТА! Точность ≥ 0.74\")\n",
        "    else:\n",
        "        print(f\"\\n⚠️  Цель не достигнута. Максимальная точность: {best_result['accuracy']:.4f}\")\n",
        "\n",
        "    # Визуализация\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    top_10 = results_df.head(10)\n",
        "    colors = ['#2E86AB' if acc >= 0.74 else '#A23B72' for acc in top_10['accuracy']]\n",
        "\n",
        "    bars = plt.barh(range(len(top_10)), top_10['accuracy'], color=colors)\n",
        "    plt.yticks(range(len(top_10)), top_10['vectorizer'])\n",
        "    plt.xlabel('Accuracy')\n",
        "    plt.title('Топ-10 конфигураций векторайзеров')\n",
        "    plt.xlim(0, 1)\n",
        "\n",
        "    # Добавляем линию цели\n",
        "    plt.axvline(x=0.74, color='red', linestyle='--', alpha=0.7, label='Цель: 0.74')\n",
        "    plt.legend()\n",
        "\n",
        "    # Добавляем значения на график\n",
        "    for i, (acc, bar) in enumerate(zip(top_10['accuracy'], bars)):\n",
        "        plt.text(acc + 0.01, bar.get_y() + bar.get_height()/2,\n",
        "                f'{acc:.3f}', va='center')\n",
        "\n",
        "    plt.grid(axis='x', alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"Нет результатов для анализа. Проверьте данные и конфигурации.\")"
      ],
      "metadata": {
        "id": "8ztbV_rv8lgL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "155d2242-48a5-4dc9-e4a2-40846c294f6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Размер обучающей выборки: 63000\n",
            "Размер тестовой выборки: 27000\n",
            "\n",
            "Количество примеров по классам:\n",
            "sentiment\n",
            "positive    21000\n",
            "neautral    21000\n",
            "negative    21000\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Соотношение классов:\n",
            "sentiment\n",
            "positive    0.333333\n",
            "neautral    0.333333\n",
            "negative    0.333333\n",
            "Name: proportion, dtype: float64\n",
            "Начинаем оценку конфигураций...\n",
            "============================================================\n",
            "\n",
            "Оценка Count (1-1, max_df=0.95)...\n",
            "  Создано признаков: 10000\n",
            "  Размерность данных: (63000, 10000)\n",
            "  Accuracy: 0.7021\n",
            "--------------------------------------------------\n",
            "\n",
            "Оценка Count (1-1, min_df=5)...\n",
            "  Создано признаков: 5774\n",
            "  Размерность данных: (63000, 5774)\n",
            "  Accuracy: 0.7014\n",
            "--------------------------------------------------\n",
            "\n",
            "Оценка Count (1-2, max_features=15000)...\n",
            "  Создано признаков: 15000\n",
            "  Размерность данных: (63000, 15000)\n",
            "  Accuracy: 0.7112\n",
            "--------------------------------------------------\n",
            "\n",
            "Оценка Count (2-2, only bigrams)...\n",
            "  Создано признаков: 5000\n",
            "  Размерность данных: (63000, 5000)\n",
            "  Accuracy: 0.6669\n",
            "--------------------------------------------------\n",
            "\n",
            "Оценка Count (1-3, max_features=20000)...\n",
            "  Создано признаков: 20000\n",
            "  Размерность данных: (63000, 20000)\n",
            "  Accuracy: 0.7098\n",
            "--------------------------------------------------\n",
            "\n",
            "Оценка Count (2-3, no unigrams)...\n",
            "  Создано признаков: 10000\n",
            "  Размерность данных: (63000, 10000)\n",
            "  Accuracy: 0.6694\n",
            "--------------------------------------------------\n",
            "\n",
            "Оценка TF-IDF (1-1)...\n",
            "  Создано признаков: 10000\n",
            "  Размерность данных: (63000, 10000)\n",
            "  Accuracy: 0.7012\n",
            "--------------------------------------------------\n",
            "\n",
            "Оценка TF-IDF (1-1, sublinear)...\n",
            "  Создано признаков: 5000\n",
            "  Размерность данных: (63000, 5000)\n",
            "  Accuracy: 0.7010\n",
            "--------------------------------------------------\n",
            "\n",
            "Оценка TF-IDF (1-2)...\n",
            "  Создано признаков: 15000\n",
            "  Размерность данных: (63000, 15000)\n",
            "  Accuracy: 0.7130\n",
            "--------------------------------------------------\n",
            "\n",
            "Оценка TF-IDF (1-2, sublinear)...\n",
            "  Создано признаков: 10000\n",
            "  Размерность данных: (63000, 10000)\n",
            "  Accuracy: 0.7125\n",
            "--------------------------------------------------\n",
            "\n",
            "Оценка TF-IDF (1-3)...\n",
            "  Создано признаков: 20000\n",
            "  Размерность данных: (63000, 20000)\n",
            "  Accuracy: 0.7114\n",
            "--------------------------------------------------\n",
            "\n",
            "Оценка Char (2-4)...\n",
            "  Создано признаков: 10000\n",
            "  Размерность данных: (63000, 10000)\n",
            "  Accuracy: 0.6804\n",
            "--------------------------------------------------\n",
            "\n",
            "Оценка Char (3-5)...\n",
            "  Создано признаков: 8000\n",
            "  Размерность данных: (63000, 8000)\n",
            "  Accuracy: 0.6815\n",
            "--------------------------------------------------\n",
            "\n",
            "Оценка Char (3-6)...\n",
            "  Создано признаков: 15000\n",
            "  Размерность данных: (63000, 15000)\n",
            "  Accuracy: 0.6867\n",
            "--------------------------------------------------\n",
            "\n",
            "Оценка Char (4-7)...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание DataFrame с результатами\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Сортируем по accuracy\n",
        "results_df = results_df.sort_values('accuracy', ascending=False)\n",
        "\n",
        "print(\"Лучшие результаты:\")\n",
        "print(results_df[['vectorizer', 'accuracy']].head())\n",
        "\n",
        "print(\"\\nПолная таблица результатов:\")\n",
        "pd.set_option('display.max_columns', None)\n",
        "print(results_df)\n",
        "\n",
        "# Визуализация\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.barh(results_df['vectorizer'], results_df['accuracy'])\n",
        "plt.xlabel('Accuracy')\n",
        "plt.title('Сравнение точности разных векторайзеров')\n",
        "plt.xlim(0, 1)\n",
        "plt.grid(axis='x', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Проверка достижения целевой точности\n",
        "best_accuracy = results_df['accuracy'].max()\n",
        "if best_accuracy >= 0.74:\n",
        "    print(f\"✓ Цель достигнута! Лучшая точность: {best_accuracy:.4f}\")\n",
        "else:\n",
        "    print(f\"✗ Цель не достигнута. Лучшая точность: {best_accuracy:.4f}\")\n",
        "    print(\"Попробуйте настроить гиперпараметры или изменить предобработку.\")"
      ],
      "metadata": {
        "id": "dmmjgzhWKGAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Грид-серч для настройки гиперпараметров лучшего векторайзера\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Используем лучший векторайзер\n",
        "best_vec_name = results_df.iloc[0]['vectorizer']\n",
        "\n",
        "if 'TF-IDF' in best_vec_name:\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    param_grid = {\n",
        "        'ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
        "        'max_df': [0.8, 0.85, 0.9],\n",
        "        'min_df': [1, 2, 3],\n",
        "        'max_features': [3000, 5000, 8000]\n",
        "    }\n",
        "elif 'Char' in best_vec_name:\n",
        "    vectorizer = CountVectorizer(analyzer='char')\n",
        "    param_grid = {\n",
        "        'ngram_range': [(2, 4), (2, 5), (3, 5), (3, 6)],\n",
        "        'max_df': [0.85, 0.9, 0.95],\n",
        "        'min_df': [1, 2],\n",
        "        'max_features': [2000, 3000, 4000]\n",
        "    }\n",
        "else:\n",
        "    vectorizer = CountVectorizer()\n",
        "    param_grid = {\n",
        "        'ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
        "        'max_df': [0.8, 0.85, 0.9],\n",
        "        'min_df': [2, 3, 4],\n",
        "        'max_features': [5000, 8000, 10000]\n",
        "    }\n",
        "\n",
        "# Векторизация для грид-серча\n",
        "X_vec = vectorizer.fit_transform(X)\n",
        "X_train_vec, X_test_vec, y_train, y_test = train_test_split(X_vec, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Поиск лучших параметров\n",
        "grid_search = GridSearchCV(\n",
        "    MultinomialNB(),\n",
        "    {'alpha': [0.1, 0.5, 1.0, 2.0]},\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train_vec, y_train)\n",
        "\n",
        "print(f\"Лучшие параметры: {grid_search.best_params_}\")\n",
        "print(f\"Лучшая точность на кросс-валидации: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Оценка на тестовой выборке\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test_vec)\n",
        "final_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Точность на тестовой выборке: {final_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "5gn8sJeBKInU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Порядок защиты работы"
      ],
      "metadata": {
        "id": "kNKi8fn1X0Qa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Для защиты работы необходимо:**\n",
        "1. Предоставить результаты выполнения задания в виде ссылки на Google Colab.\n",
        "2. Ответить на контрольные вопросы по содержанию работы.\n",
        "\n",
        "**Примеры контрольных вопросов:**\n",
        "1. Какие шаги предобработки из приведенного в задании списка не использовались в вашей работе? Почему?\n",
        "2. Объясните отличие стемминга от лемматизации. Какой вид нормализации текста использовался в вашей работе?\n",
        "3. Как влияет указание конкретного значения `max_features` на формирование вектора документа через `TfidfVectorizer()`?\n",
        "4. В чем разница между `accuracy` и `precision`?\n",
        "5. Почему для классификации текстов в этой лабораторной логично применять `MultinomialNB`, а не другую вариацию наивного байесовского классификатора из `sklearn`?"
      ],
      "metadata": {
        "id": "vNArFxJcX8uS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ответы на контрольные вопросы\n",
        "1) Какие шаги предобработки не использовались? Почему?\n",
        "\n",
        "Я не делал исправление опечаток и какие-то сложные этапы типа “понимания смысла” слов.\n",
        "Опечатки можно пытаться исправлять, но это иногда наоборот портит текст (особенно в отзывах, где много разговорных слов). Плюс у меня были символьные n-граммы, они обычно нормально переживают ошибки в словах.\n",
        "\n",
        "2) Отличие стемминга от лемматизации. Что использовал я?\n",
        "\n",
        "Стемминг — это когда у слова просто “отрезают” конец, и получается кусок слова. Быстро, но иногда получается странно.\n",
        "Лемматизация — это когда слово приводят к нормальной форме из словаря (например “кошке” → “кошка”). Это точнее.\n",
        "\n",
        "Я использовал лемматизацию через pymorphy2, потому что для русского языка так обычно лучше и понятнее получается.\n",
        "\n",
        "3) Как влияет max_features в TfidfVectorizer()?\n",
        "\n",
        "max_features — это ограничение на количество признаков (слов/нграмм), которые попадут в модель.\n",
        "\n",
        "Если поставить меньше:\n",
        "\n",
        "модель быстрее обучается,\n",
        "\n",
        "меньше памяти,\n",
        "\n",
        "иногда даже лучше качество (меньше “шума”).\n",
        "\n",
        "Но если поставить слишком мало — можно выкинуть полезные слова, и качество упадёт.\n",
        "\n",
        "4) В чем разница между accuracy и precision?\n",
        "\n",
        "Accuracy — это “сколько всего угадали правильно” из всех примеров.\n",
        "\n",
        "Precision — это “насколько можно доверять предсказаниям класса”.\n",
        "Например, если модель сказала “positive”, precision показывает, сколько из них реально positive.\n",
        "\n",
        "5) Почему лучше MultinomialNB, а не другие Naive Bayes?\n",
        "\n",
        "Потому что MultinomialNB хорошо работает именно с текстами, где признаки — это количество слов или TF-IDF, то есть неотрицательные значения.\n",
        "\n",
        "GaussianNB больше для чисел, которые похожи на нормальное распределение (не наш случай).\n",
        "\n",
        "BernoulliNB лучше, когда признаки 0/1 (есть слово или нет), но при частотах и TF-IDF обычно хуже подходит."
      ],
      "metadata": {
        "id": "3N-U-PgSK7Ii"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Дополнительные материалы"
      ],
      "metadata": {
        "id": "o5oPWAt-Z_s_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Автоматическая обработка текстов на естественном языке и анализ данных : учеб. пособие / Большакова Е.И., Воронцов К.В., Ефремова Н.Э., Клышинский Э.С., Лукашевич Н.В., Сапин А.С. — М.: Изд-во НИУ ВШЭ, 2017. — 269 с.\n",
        "1. Naive Bayes. Scikit-learn [Электронный ресурс]. URL: https://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes (дата обращения: 02.04.2024)."
      ],
      "metadata": {
        "id": "-MwwS_lTaSGM"
      }
    }
  ]
}